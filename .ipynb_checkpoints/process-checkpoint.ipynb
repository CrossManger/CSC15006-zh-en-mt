{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6f9e3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pysbd\n",
    "import json\n",
    "import re\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "294927c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def segment_text(text, lang):\n",
    "    seg = pysbd.Segmenter(language=lang, clean=False)\n",
    "    return seg.segment(text)\n",
    "\n",
    "def preprocess_data(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for item in data:\n",
    "        input_text = item['input']\n",
    "        output_text = item['output']\n",
    "\n",
    "        input_text = re.sub(r'(?<=\\.)(?=[a-z0-9]+\\.)', '\\n', input_text)\n",
    "        output_text = re.sub(r'(?<=\\.)(?=[a-z0-9]+\\.)', '\\n', output_text)\n",
    "\n",
    "        input_sentences = segment_text(input_text, 'zh')\n",
    "        output_sentences = segment_text(output_text, 'en')\n",
    "\n",
    "        processed_item = {\n",
    "            'input': input_sentences,\n",
    "            'output': output_sentences,\n",
    "            'id': item['id']\n",
    "        }\n",
    "        processed_data.append(processed_item)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfadf3a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_data(\"data/scidb_cn_zh_en.json\", \"data/processed_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf1cc42",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# embedding.py\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import config\n",
    "\n",
    "def get_model():\n",
    "    print(f\"Loading model: {config.MODEL_NAME}...\")\n",
    "    return SentenceTransformer(config.MODEL_NAME)\n",
    "\n",
    "def create_embeddings(model, sentences):\n",
    "    \"\"\"\n",
    "    Input: List of strings\n",
    "    Output: Numpy array (float32)\n",
    "    \"\"\"\n",
    "    print(f\"Encoding {len(sentences)} sentences...\")\n",
    "    # Encode batch\n",
    "    embeddings = model.encode(\n",
    "        sentences, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        show_progress_bar=True, \n",
    "        convert_to_numpy=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "def save_binary(embeddings, filepath):\n",
    "    \"\"\"\n",
    "    Lưu numpy array xuống file binary chuẩn float32 cho Vecalign\n",
    "    \"\"\"\n",
    "    # Vecalign bắt buộc input là raw float32\n",
    "    embeddings.astype('float32').tofile(filepath)\n",
    "    print(f\"Saved binary embeddings to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73fb74d2-6558-4023-b13a-0be9d5108ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records to process: 10\n",
      "--- Preprocessing & Segmenting ---\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocess' has no attribute 'segment_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m rec_id \u001b[38;5;241m=\u001b[39m rec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Tách câu\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m src_seg \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msegment_text\u001b[49m(rec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh_text\u001b[39m\u001b[38;5;124m'\u001b[39m], lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m tgt_seg \u001b[38;5;241m=\u001b[39m preprocess\u001b[38;5;241m.\u001b[39msegment_text(rec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_text\u001b[39m\u001b[38;5;124m'\u001b[39m], lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Add vào list tổng\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'preprocess' has no attribute 'segment_text'"
     ]
    }
   ],
   "source": [
    "# prepare_vecalign.py\n",
    "import json\n",
    "import config\n",
    "import preprocess\n",
    "import embedding\n",
    "\n",
    "def load_data():\n",
    "    # TODO: Thay hàm này bằng code đọc file json thật của bạn\n",
    "    with open(\"data\\processed_data.json\", 'r', encoding='utf-8') as f:\n",
    "        dat = json.load(f)\n",
    "    return dat\n",
    "\n",
    "# 1. Load Data\n",
    "records = load_data() # Thay bằng hàm load file thật\n",
    "print(f\"Total records to process: {len(records)}\")\n",
    "\n",
    "# Lists tổng chứa toàn bộ corpus đã stitch\n",
    "all_src_sents = []\n",
    "all_tgt_sents = []\n",
    "\n",
    "# List lưu map để sau này biết câu thứ i thuộc record nào\n",
    "# Format: (sentence_index, record_id, is_barrier)\n",
    "src_map = [] \n",
    "\n",
    "# 2. Preprocess & Stitching (Khâu dữ liệu & Chèn Rào chắn)\n",
    "print(\"--- Preprocessing & Segmenting ---\")\n",
    "\n",
    "for rec in records:\n",
    "    rec_id = rec['id']\n",
    "    \n",
    "    # Tách câu\n",
    "    src_seg = preprocess.segment_text(rec['zh_text'], lang='zh')\n",
    "    tgt_seg = preprocess.segment_text(rec['en_text'], lang='en')\n",
    "    \n",
    "    # Add vào list tổng\n",
    "    for s in src_seg:\n",
    "        all_src_sents.append(s)\n",
    "        src_map.append(f\"{rec_id}\") # Lưu ID\n",
    "        \n",
    "    for s in tgt_seg:\n",
    "        all_tgt_sents.append(s)\n",
    "        \n",
    "    # --- CRITICAL: CHÈN BARRIER ---\n",
    "    # Chèn vào cả 2 bên để vecalign neo lại tại đây\n",
    "    all_src_sents.append(config.BARRIER_TOKEN)\n",
    "    all_tgt_sents.append(config.BARRIER_TOKEN)\n",
    "    src_map.append(\"BARRIER\") # Đánh dấu đây là rào chắn\n",
    "\n",
    "# 3. Export Text Files (Vecalign cần text để tham chiếu dòng)\n",
    "print(\"--- Saving Text Files ---\")\n",
    "with open(config.SRC_TEXT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(all_src_sents))\n",
    "    \n",
    "with open(config.TGT_TEXT_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(all_tgt_sents))\n",
    "    \n",
    "with open(config.MAP_FILE, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(src_map))\n",
    "\n",
    "# 4. Generate Embeddings\n",
    "print(\"--- Generating Embeddings ---\")\n",
    "model = embedding.get_model()\n",
    "\n",
    "src_emb = embedding.create_embeddings(model, all_src_sents)\n",
    "tgt_emb = embedding.create_embeddings(model, all_tgt_sents)\n",
    "\n",
    "# 5. Save Binary Files\n",
    "embedding.save_binary(src_emb, config.SRC_EMB_FILE)\n",
    "embedding.save_binary(tgt_emb, config.TGT_EMB_FILE)\n",
    "\n",
    "print(\"\\n[DONE] Data prepared successfully!\")\n",
    "print(f\"Dimensions: {src_emb.shape}\")\n",
    "print(\"You can now run 'sh run_vecalign.sh'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ac3d5b-6c71-48ba-98d3-da5597a55bda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
